{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "794ad0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import norm\n",
    "import statistics\n",
    "import seaborn as sns\n",
    "from scipy.stats import f_oneway\n",
    "import scikit_posthocs as sp\n",
    "from scipy.stats import kruskal\n",
    "import statsmodels.stats.multicomp as mc\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c718ab8",
   "metadata": {},
   "source": [
    "# Importing data\n",
    "- This notebook will classify the different claims with the help of the significant features.\n",
    "- We import the computed Liar data frame and conduct several classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c67ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('/Users/sandrobarreshamers/Thesis_IS_fake_news/ThesisData/Liar_computed_final_version.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb31faab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'json_id', 'claim', 'object', 'binary label',\n",
       "       'readability', 'compressed_size', 'vader_neg', 'vader_neu', 'vader_pos',\n",
       "       'vader_compound', 'tot_ner_count', 'ner_counts', 'input_vector_ner',\n",
       "       'NER_CARDINAL', 'NER_DATE', 'NER_EVENT', 'NER_FAC', 'NER_GPE',\n",
       "       'NER_LANGUAGE', 'NER_LAW', 'NER_LOC', 'NER_MONEY', 'NER_NORP',\n",
       "       'NER_ORDINAL', 'NER_ORG', 'NER_PERCENT', 'NER_PERSON', 'NER_PRODUCT',\n",
       "       'NER_QUANTITY', 'NER_TIME', 'NER_WORK_OF_ART', 'pos counts',\n",
       "       'input_vector_pos', 'pos_ADJ', 'pos_ADP', 'pos_ADV', 'pos_AUX',\n",
       "       'pos_CCONJ', 'pos_DET', 'pos_INTJ', 'pos_NOUN', 'pos_NUM', 'pos_PART',\n",
       "       'pos_PRON', 'pos_PROPN', 'pos_PUNCT', 'pos_SCONJ', 'pos_SYM',\n",
       "       'pos_VERB', 'pos_X'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c377c8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83328ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>binary_label</th>\n",
       "      <th>statement</th>\n",
       "      <th>readability</th>\n",
       "      <th>compressed_size</th>\n",
       "      <th>vader_neg</th>\n",
       "      <th>vader_neu</th>\n",
       "      <th>vader_pos</th>\n",
       "      <th>vader_compound</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_NOUN</th>\n",
       "      <th>pos_NUM</th>\n",
       "      <th>pos_PART</th>\n",
       "      <th>pos_PRON</th>\n",
       "      <th>pos_PROPN</th>\n",
       "      <th>pos_PUNCT</th>\n",
       "      <th>pos_SCONJ</th>\n",
       "      <th>pos_SYM</th>\n",
       "      <th>pos_VERB</th>\n",
       "      <th>pos_X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11972.json</td>\n",
       "      <td>0</td>\n",
       "      <td>Building a wall on the U.S.-Mexico border will...</td>\n",
       "      <td>49.542727</td>\n",
       "      <td>5197</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11685.json</td>\n",
       "      <td>1</td>\n",
       "      <td>Wisconsin is on pace to double the number of l...</td>\n",
       "      <td>81.855000</td>\n",
       "      <td>5548</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.894</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.0772</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>11096.json</td>\n",
       "      <td>1</td>\n",
       "      <td>Says John McCain has done nothing to help the ...</td>\n",
       "      <td>103.625000</td>\n",
       "      <td>4816</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.3089</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5209.json</td>\n",
       "      <td>2</td>\n",
       "      <td>Suzanne Bonamici supports a plan that will cut...</td>\n",
       "      <td>43.963077</td>\n",
       "      <td>6020</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.3400</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9524.json</td>\n",
       "      <td>1</td>\n",
       "      <td>When asked by a reporter whether hes at the ce...</td>\n",
       "      <td>62.625000</td>\n",
       "      <td>10724</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.5994</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0          id  binary_label  \\\n",
       "0           0  11972.json             0   \n",
       "1           1  11685.json             1   \n",
       "2           2  11096.json             1   \n",
       "3           3   5209.json             2   \n",
       "4           4   9524.json             1   \n",
       "\n",
       "                                           statement  readability  \\\n",
       "0  Building a wall on the U.S.-Mexico border will...    49.542727   \n",
       "1  Wisconsin is on pace to double the number of l...    81.855000   \n",
       "2  Says John McCain has done nothing to help the ...   103.625000   \n",
       "3  Suzanne Bonamici supports a plan that will cut...    43.963077   \n",
       "4  When asked by a reporter whether hes at the ce...    62.625000   \n",
       "\n",
       "   compressed_size  vader_neg  vader_neu  vader_pos  vader_compound  ...  \\\n",
       "0             5197      0.000      1.000      0.000          0.0000  ...   \n",
       "1             5548      0.000      0.894      0.106          0.0772  ...   \n",
       "2             4816      0.201      0.799      0.000         -0.3089  ...   \n",
       "3             6020      0.127      0.602      0.271          0.3400  ...   \n",
       "4            10724      0.225      0.683      0.092         -0.5994  ...   \n",
       "\n",
       "   pos_NOUN pos_NUM pos_PART  pos_PRON  pos_PROPN  pos_PUNCT  pos_SCONJ  \\\n",
       "0       3.0     0.0      0.0       0.0        1.0        1.0        0.0   \n",
       "1       4.0     0.0      1.0       0.0        1.0        1.0        0.0   \n",
       "2       1.0     0.0      1.0       1.0        2.0        1.0        0.0   \n",
       "3       3.0     0.0      0.0       0.0        4.0        1.0        0.0   \n",
       "4       5.0     0.0      1.0       1.0        3.0        2.0        1.0   \n",
       "\n",
       "   pos_SYM  pos_VERB  pos_X  \n",
       "0      0.0       3.0    0.0  \n",
       "1      0.0       1.0    0.0  \n",
       "2      0.0       3.0    0.0  \n",
       "3      0.0       3.0    0.0  \n",
       "4      0.0       4.0    0.0  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59618bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'id', 'binary_label', 'statement', 'readability',\n",
       "       'compressed_size', 'vader_neg', 'vader_neu', 'vader_pos',\n",
       "       'vader_compound', 'tot_ner_count', 'ner_counts', 'input_vector_ner',\n",
       "       'NER_CARDINAL', 'NER_DATE', 'NER_EVENT', 'NER_FAC', 'NER_GPE',\n",
       "       'NER_LANGUAGE', 'NER_LAW', 'NER_LOC', 'NER_MONEY', 'NER_NORP',\n",
       "       'NER_ORDINAL', 'NER_ORG', 'NER_PERCENT', 'NER_PERSON', 'NER_PRODUCT',\n",
       "       'NER_QUANTITY', 'NER_TIME', 'NER_WORK_OF_ART', 'pos counts',\n",
       "       'input_vector_pos', 'pos_ADJ', 'pos_ADP', 'pos_ADV', 'pos_AUX',\n",
       "       'pos_CCONJ', 'pos_DET', 'pos_INTJ', 'pos_NOUN', 'pos_NUM', 'pos_PART',\n",
       "       'pos_PRON', 'pos_PROPN', 'pos_PUNCT', 'pos_SCONJ', 'pos_SYM',\n",
       "       'pos_VERB', 'pos_X'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89d18e8",
   "metadata": {},
   "source": [
    "## Classification pre-face\n",
    "- in this code blocks, we prepare the data and set up the experiment to later classify the False and True label\n",
    "- Based on the previously conducted KST and Dunn tests, we now train the different models on the significant features\n",
    "- We divide the data in train and test data\n",
    "- we instantiate the K-fold cross validation for both the train and test data with k=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e3c3733",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_feature_selection = ['readability', 'compressed_size', 'vader_neg', 'vader_neu', 'vader_pos',\n",
    "       'vader_compound', 'tot_ner_count',\n",
    "       'NER_CARDINAL', 'NER_DATE', 'NER_EVENT', 'NER_FAC', 'NER_GPE',\n",
    "       'NER_LANGUAGE', 'NER_LAW', 'NER_LOC', 'NER_MONEY', 'NER_NORP',\n",
    "       'NER_ORDINAL', 'NER_ORG', 'NER_PERCENT', 'NER_PERSON', 'NER_PRODUCT',\n",
    "       'NER_QUANTITY', 'NER_TIME', 'NER_WORK_OF_ART', 'pos_ADJ', 'pos_ADP', 'pos_ADV', 'pos_AUX',\n",
    "       'pos_CCONJ', 'pos_DET', 'pos_INTJ', 'pos_NOUN', 'pos_NUM', 'pos_PART',\n",
    "       'pos_PRON', 'pos_PROPN', 'pos_PUNCT', 'pos_SCONJ', 'pos_SYM',\n",
    "       'pos_VERB', 'pos_X']\n",
    "\n",
    "all_significant_features = ['readability', 'compressed_size', 'vader_neg', 'vader_neu', 'tot_ner_count','NER_CARDINAL', 'NER_DATE', 'NER_MONEY', 'NER_ORDINAL', 'NER_ORG', 'NER_PERCENT', 'NER_PERSON','NER_WORK_OF_ART',\n",
    "                            'pos_ADJ', 'pos_ADP', 'pos_ADV', 'pos_AUX', 'pos_CCONJ', 'pos_DET', 'pos_NOUN', 'pos_NUM', 'pos_PART', 'pos_PROPN', 'pos_PUNCT', 'pos_SCONJ', 'pos_SYM', 'pos_VERB']\n",
    "\n",
    "\n",
    "three_way_significant_features= ['readability', 'compressed_size','NER_CARDINAL','NER_PERSON','pos_AUX','pos_PART','pos_VERB']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d84a41",
   "metadata": {},
   "source": [
    "## Classification Algorithms\n",
    "- In the following code blocks, we perform the three ML classification algorithms discussed in the research paper; the three models are:\n",
    "  - Naive Bias\n",
    "  - Random Forest\n",
    "  - Gradient Booster \n",
    "  - Dummy classifier\n",
    "  \n",
    "- The three models are trained and evaluated on the three feature inputs\n",
    "  - No feature selection\n",
    "  - Two-way significant feature selection\n",
    "  - Three-way significant feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6f47dd",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1941ac57",
   "metadata": {},
   "source": [
    "### Naive Bayes no feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d880747a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score k-fold cross-validation on training data for no_feature_selection: 0.37, cv:(+/- 0.05)\n",
      "F1 score k-fold cross-validation on test data for no_feature_selection: 0.34, cv:(+/- 0.06)\n",
      "F1 score k-fold cross-validation on training data for all_significant_features: 0.37, cv:(+/- 0.04)\n",
      "F1 score k-fold cross-validation on test data for all_significant_features: 0.33, cv:(+/- 0.11)\n",
      "F1 score k-fold cross-validation on training data for three_way_significant_features: 0.36, cv:(+/- 0.05)\n",
      "F1 score k-fold cross-validation on test data for three_way_significant_features: 0.27, cv:(+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "\n",
    "# Define the feature sets and their names\n",
    "feature_selections = [\n",
    "    (no_feature_selection, 'no_feature_selection'),\n",
    "    (all_significant_features, 'all_significant_features'),\n",
    "    (three_way_significant_features, 'three_way_significant_features')\n",
    "]\n",
    "\n",
    "NB_CMs = {'cm_NB_all': None, 'cm_NB_s': None, 'cm_NB_three': None}\n",
    "NB_k_fold_cross_validations_train = []\n",
    "NB_k_fold_cross_validations_test = []\n",
    "\n",
    "# Define a custom scoring function for F1 score\n",
    "scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "for i, (features, feature_name) in enumerate(feature_selections):\n",
    "    X_train_selected = train_data[features]\n",
    "    y_train = train_data['binarylabel']\n",
    "    \n",
    "    X_test_selected = test_data[features]\n",
    "    y_test = test_data['binary_label']\n",
    "    \n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = ros.fit_resample(X_train_selected, y_train)\n",
    "\n",
    "    nb_model = GaussianNB()\n",
    "    nb_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    y_pred_train = nb_model.predict(X_train_resampled)\n",
    "    y_pred_test = nb_model.predict(X_test_selected)\n",
    "\n",
    "    cm_NB_train = confusion_matrix(y_train_resampled, y_pred_train)\n",
    "    cm_NB_test = confusion_matrix(y_test, y_pred_test)\n",
    "    NB_CMs[list(NB_CMs.keys())[i]] = {'train': cm_NB_train, 'test': cm_NB_test}\n",
    "\n",
    "    kfold_train = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores_train = cross_val_score(nb_model, X_train_resampled, y_train_resampled, cv=kfold_train, scoring=scorer)\n",
    "    print(f\"F1 score k-fold cross-validation on training data for {feature_name}: {scores_train.mean():.2f}, cv:(+/- {scores_train.std()/scores_train.mean():.2f})\")\n",
    "    NB_k_fold_cross_validations_train.append(scores_train)\n",
    "\n",
    "    kfold_test = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores_test = cross_val_score(nb_model, X_test_selected, y_test, cv=kfold_test, scoring=scorer)\n",
    "    print(f\"F1 score k-fold cross-validation on test data for {feature_name}: {scores_test.mean():.2f}, cv:(+/- {scores_test.std()/scores_test.mean():.2f})\")\n",
    "    NB_k_fold_cross_validations_test.append(scores_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d3d59d",
   "metadata": {},
   "source": [
    "## Random forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d87bb94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score k-fold cross-validation on training data for no_feature_selection: 0.84, cv:(+/- 0.01)\n",
      "F1 score k-fold cross-validation on test data for no_feature_selection: 0.28, cv:(+/- 0.08)\n",
      "F1 score k-fold cross-validation on training data for all_significant_features: 0.84, cv:(+/- 0.01)\n",
      "F1 score k-fold cross-validation on test data for all_significant_features: 0.29, cv:(+/- 0.06)\n",
      "F1 score k-fold cross-validation on training data for three_way_significant_features: 0.77, cv:(+/- 0.01)\n",
      "F1 score k-fold cross-validation on test data for three_way_significant_features: 0.32, cv:(+/- 0.06)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "RF_CMs = {'cm_RF_all': None, 'cm_RF_s': None, 'cm_RF_three': None}\n",
    "RF_k_fold_cross_validations_train = []\n",
    "RF_k_fold_cross_validations_test = []\n",
    "\n",
    "for i, (features, feature_name) in enumerate(feature_selections):\n",
    "    X_train_selected = train_data[features]\n",
    "    y_train = train_data['binary label']\n",
    "    \n",
    "    X_test_selected = test_data[features]\n",
    "    y_test = test_data['binary_label']\n",
    "    \n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = ros.fit_resample(X_train_selected, y_train)\n",
    "\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    y_pred_train = rf_model.predict(X_train_resampled)\n",
    "    y_pred_test = rf_model.predict(X_test_selected)\n",
    "\n",
    "    cm_RF_train = confusion_matrix(y_train_resampled, y_pred_train)\n",
    "    cm_RF_test = confusion_matrix(y_test, y_pred_test)\n",
    "    RF_CMs[list(RF_CMs.keys())[i]] = {'train': cm_RF_train, 'test': cm_RF_test}\n",
    "\n",
    "    kfold_train = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores_train = cross_val_score(rf_model, X_train_resampled, y_train_resampled, cv=kfold_train, scoring=scorer)\n",
    "    print(f\"F1 score k-fold cross-validation on training data for {feature_name}: {scores_train.mean():.2f}, cv:(+/- {scores_train.std()/scores_train.mean():.2f})\")\n",
    "    RF_k_fold_cross_validations_train.append(scores_train)\n",
    "\n",
    "    kfold_test = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores_test = cross_val_score(rf_model, X_test_selected, y_test, cv=kfold_test, scoring=scorer)\n",
    "    print(f\"F1 score k-fold cross-validation on test data for {feature_name}: {scores_test.mean():.2f}, cv:(+/- {scores_test.std()/scores_test.mean():.2f})\")\n",
    "    RF_k_fold_cross_validations_test.append(scores_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29af9d59",
   "metadata": {},
   "source": [
    "## Gradient Booster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eeb850e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score k-fold cross-validation on training data for no_feature_selection: 0.44, cv:(+/- 0.02)\n",
      "F1 score k-fold cross-validation on test data for no_feature_selection: 0.30, cv:(+/- 0.10)\n",
      "F1 score k-fold cross-validation on training data for all_significant_features: 0.44, cv:(+/- 0.02)\n",
      "F1 score k-fold cross-validation on test data for all_significant_features: 0.31, cv:(+/- 0.13)\n",
      "F1 score k-fold cross-validation on training data for three_way_significant_features: 0.42, cv:(+/- 0.02)\n",
      "F1 score k-fold cross-validation on test data for three_way_significant_features: 0.29, cv:(+/- 0.07)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "GB_CMs = {'cm_GB_all': None, 'cm_GB_s': None, 'cm_GB_three': None}\n",
    "GB_k_fold_cross_validations_train = []\n",
    "GB_k_fold_cross_validations_test = []\n",
    "\n",
    "\n",
    "for i, (features, feature_name) in enumerate(feature_selections):\n",
    "    X_train_selected = train_data[features]\n",
    "    y_train = train_data['binary label']\n",
    "    \n",
    "    X_test_selected = test_data[features]\n",
    "    y_test = test_data['binary_label']\n",
    "    \n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = ros.fit_resample(X_train_selected, y_train)\n",
    "\n",
    "    gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.08, max_depth=2, subsample=0.5, random_state=42)\n",
    "    gb_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    y_pred_train = gb_model.predict(X_train_resampled)\n",
    "    y_pred_test = gb_model.predict(X_test_selected)\n",
    "\n",
    "    cm_GB_train = confusion_matrix(y_train_resampled, y_pred_train)\n",
    "    cm_GB_test = confusion_matrix(y_test, y_pred_test)\n",
    "    GB_CMs[list(GB_CMs.keys())[i]] = {'train': cm_GB_train, 'test': cm_GB_test}\n",
    "\n",
    "    kfold_train = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores_train = cross_val_score(gb_model, X_train_resampled, y_train_resampled, cv=kfold_train, scoring=scorer)\n",
    "    print(f\"F1 score k-fold cross-validation on training data for {feature_name}: {scores_train.mean():.2f}, cv:(+/- {scores_train.std()/scores_train.mean():.2f})\")\n",
    "    GB_k_fold_cross_validations_train.append(scores_train)\n",
    "\n",
    "    kfold_test = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores_test = cross_val_score(gb_model, X_test_selected, y_test, cv=kfold_test, scoring=scorer)\n",
    "    print(f\"F1 score k-fold cross-validation on test data for {feature_name}: {scores_test.mean():.2f}, cv:(+/- {scores_test.std()/scores_test.mean():.2f})\")\n",
    "    GB_k_fold_cross_validations_test.append(scores_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bbc800",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4d5fabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"uniform\")\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "y_pred_uniform = dummy_clf.predict(X_test)\n",
    "accuracy_stratified = accuracy_score(y_test, y_pred_uniform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c6c3e247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score k fold cross training data:0.33 (+/- 0.00)\n",
      "f1 score k fold cross for the test data: 0.32 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "scores_Dummy_train = cross_val_score(dummy_clf, X_train_resampled, y_train_resampled, cv=kfold_train, scoring='f1_macro')\n",
    "print(\"f1 score k fold cross training data:%0.2f (+/- %0.2f)\" % (scores_Dummy_train.mean(), scores_Dummy_train.std()))\n",
    "\n",
    "#Test\n",
    "scores_Dummy_test = cross_val_score(dummy_clf, X_test, y_test, cv=kfold_train, scoring='f1_macro')\n",
    "print(\"f1 score k fold cross for the test data: %0.2f (+/- %0.2f)\" % (scores_Dummy_test.mean(), scores_Dummy_test.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e486d8",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "- In this code block, the different classification ML models are evaluated and compared against each other\n",
    "- Subsequently, the evaluation is displayed in a table and later printed in latex format\n",
    "- this latex format is used in the research paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8a265aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Feature Selection  F1 Score Test (%)  \\\n",
      "Model                                                                \n",
      "Naive Bayes                          Unselected   34.11 (+/- 6.44)   \n",
      "Naive Bayes                 two-way significant  33.06 (+/- 11.07)   \n",
      "Naive Bayes               three-way significant   27.35 (+/- 2.48)   \n",
      "Random Forrest                       Unselected   27.59 (+/- 7.77)   \n",
      "Random Forrest              two-way significant   29.40 (+/- 6.09)   \n",
      "Random Forrest            three-way significant   31.72 (+/- 6.49)   \n",
      "Gradient Booster                     Unselected   30.48 (+/- 9.72)   \n",
      "Gradient Booster            two-way significant  30.94 (+/- 12.86)   \n",
      "Gradient Booster          three-way significant   28.92 (+/- 6.98)   \n",
      "Dummy Classifier Uniform          Nonapplicable   31.88 (+/- 2.33)   \n",
      "\n",
      "                          Relative Improvement (%)  \n",
      "Model                                               \n",
      "Naive Bayes                                   7.00  \n",
      "Naive Bayes                                   3.71  \n",
      "Naive Bayes                                 -14.19  \n",
      "Random Forrest                              -13.44  \n",
      "Random Forrest                               -7.76  \n",
      "Random Forrest                               -0.47  \n",
      "Gradient Booster                             -4.37  \n",
      "Gradient Booster                             -2.93  \n",
      "Gradient Booster                             -9.27  \n",
      "Dummy Classifier Uniform                      0.00  \n"
     ]
    }
   ],
   "source": [
    "eval_df = pd.DataFrame({\n",
    "    'Model': model_names,\n",
    "    'Feature Selection': feature_selection,\n",
    "    'F1 Score Test (%)': [\n",
    "        f\"{np.mean(NB_k_fold_cross_validations_test[0]) * 100:.2f} (+/- {(NB_k_fold_cross_validations_test[0].std()/NB_k_fold_cross_validations_test[0].mean()) * 100:.2f})\",\n",
    "        f\"{np.mean(NB_k_fold_cross_validations_test[1]) * 100:.2f} (+/- {(NB_k_fold_cross_validations_test[1].std()/NB_k_fold_cross_validations_test[1].mean()) * 100:.2f})\",\n",
    "        f\"{np.mean(NB_k_fold_cross_validations_test[2]) * 100:.2f} (+/- {(NB_k_fold_cross_validations_test[2].std()/NB_k_fold_cross_validations_test[2].mean()) * 100:.2f})\",\n",
    "        f\"{np.mean(RF_k_fold_cross_validations_test[0]) * 100:.2f} (+/- {(RF_k_fold_cross_validations_test[0].std()/RF_k_fold_cross_validations_test[0].mean()) * 100:.2f})\",\n",
    "        f\"{np.mean(RF_k_fold_cross_validations_test[1]) * 100:.2f} (+/- {(RF_k_fold_cross_validations_test[1].std()/RF_k_fold_cross_validations_test[1].mean()) * 100:.2f})\",\n",
    "        f\"{np.mean(RF_k_fold_cross_validations_test[2]) * 100:.2f} (+/- {(RF_k_fold_cross_validations_test[2].std()/RF_k_fold_cross_validations_test[2].mean()) * 100:.2f})\",\n",
    "        f\"{np.mean(GB_k_fold_cross_validations_test[0]) * 100:.2f} (+/- {(GB_k_fold_cross_validations_test[0].std()/GB_k_fold_cross_validations_test[0].mean()) * 100:.2f})\",\n",
    "        f\"{np.mean(GB_k_fold_cross_validations_test[1]) * 100:.2f} (+/- {(GB_k_fold_cross_validations_test[1].std()/GB_k_fold_cross_validations_test[1].mean()) * 100:.2f})\",\n",
    "        f\"{np.mean(GB_k_fold_cross_validations_test[2]) * 100:.2f} (+/- {(GB_k_fold_cross_validations_test[2].std()/GB_k_fold_cross_validations_test[2].mean()) * 100:.2f})\",\n",
    "        f\"{np.mean(scores_Dummy_test) * 100:.2f} (+/- {(scores_Dummy_test.std()/scores_Dummy_test.mean()) * 100:.2f})\"\n",
    "    ],\n",
    "    'Relative Improvement (%)': [\n",
    "        round(Relative_Improvement_NB_nf, 2),\n",
    "        round(Relative_Improvement_NB_f, 2),\n",
    "        round(Relative_Improvement_NB_three_f, 2),\n",
    "        round(Relative_Improvement_RF_nf, 2),\n",
    "        round(Relative_Improvement_RF_f, 2),\n",
    "        round(Relative_Improvement_RF_three_f, 2),\n",
    "        round(Relative_Improvement_GB_nf, 2),\n",
    "        round(Relative_Improvement_GB_f, 2),\n",
    "        round(Relative_Improvement_GB_three_f, 2),\n",
    "        round(Relative_Improvement_DC, 2)\n",
    "    ]\n",
    "})\n",
    "\n",
    "eval_df.set_index('Model', inplace=True)\n",
    "print(eval_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a5f4ca31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllr}\n",
      "\\toprule\n",
      "{} &      Feature Selection &  F1 Score Test (\\%) &  Relative Improvement (\\%) \\\\\n",
      "Model                    &                        &                    &                           \\\\\n",
      "\\midrule\n",
      "Naive Bayes              &             Unselected &   34.11 (+/- 6.44) &                      7.00 \\\\\n",
      "Naive Bayes              &    two-way significant &  33.06 (+/- 11.07) &                      3.71 \\\\\n",
      "Naive Bayes              &  three-way significant &   27.35 (+/- 2.48) &                    -14.19 \\\\\n",
      "Random Forrest           &             Unselected &   27.59 (+/- 7.77) &                    -13.44 \\\\\n",
      "Random Forrest           &    two-way significant &   29.40 (+/- 6.09) &                     -7.76 \\\\\n",
      "Random Forrest           &  three-way significant &   31.72 (+/- 6.49) &                     -0.47 \\\\\n",
      "Gradient Booster         &             Unselected &   30.48 (+/- 9.72) &                     -4.37 \\\\\n",
      "Gradient Booster         &    two-way significant &  30.94 (+/- 12.86) &                     -2.93 \\\\\n",
      "Gradient Booster         &  three-way significant &   28.92 (+/- 6.98) &                     -9.27 \\\\\n",
      "Dummy Classifier Uniform &          Nonapplicable &   31.88 (+/- 2.33) &                      0.00 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0q/8tgppr255xx8v40x26xhhmhr0000gn/T/ipykernel_837/3099801597.py:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  latex_table_ML_eval = eval_df.to_latex(index=True, float_format=lambda x: \"%.2f\" % x)\n"
     ]
    }
   ],
   "source": [
    "latex_table_ML_eval = eval_df.to_latex(index=True, float_format=lambda x: \"%.2f\" % x)\n",
    "print(latex_table_ML_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a0695d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b56a52e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c83407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3028afa7",
   "metadata": {},
   "source": [
    "### End of notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
